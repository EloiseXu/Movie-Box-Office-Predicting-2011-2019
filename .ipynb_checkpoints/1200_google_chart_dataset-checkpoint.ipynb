{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping google charts using requests, scrapy & Beautifulsoup\n",
    "\n",
    "__Purpose__\n",
    "\n",
    "In this notebook google charts dataset will be scraped directly from itunes' website. Only title, description, rating, genre and the title of chart will be kept. We use Beautifulsoup instead of xpath to avoid any trouble brought by different positions of the same data in different apps' website. To scrape 100 apps' data from each chart's website, Scrapy is needed or dynamically loaded data will be missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "import time\n",
    "import random\n",
    "\n",
    "import requests\n",
    "from requests.compat import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_save_path = '../../datasets/1200_google_chart_dataset.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping charts using requests & BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Scraping charts using requests & BeautifulSoup')\n",
    "\n",
    "google_dataset = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape 100 apps' data from each chart's website, Scrapy is needed or dynamically loaded data will be missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-13 23:11:58 [scrapy.utils.log] INFO: Scrapy 1.5.2 started (bot: scrapybot)\n",
      "2019-04-13 23:11:58 [scrapy.utils.log] INFO: Versions: lxml 4.3.0.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) - [GCC 7.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1  11 Sep 2018), cryptography 2.6.1, Platform Linux-4.15.0-47-generic-x86_64-with-debian-buster-sid\n",
      "2019-04-13 23:11:58 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 'WARNING', 'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n"
     ]
    }
   ],
   "source": [
    "class Spider(scrapy.Spider):\n",
    "    name = \"playspider\"  \n",
    "    allowed_domains = ['play.google.com']\n",
    "\n",
    "    def start_requests(self): \n",
    "        start_urls = [\"https://play.google.com/store/apps/collection/topselling_free\",\n",
    "                    \"https://play.google.com/store/apps/collection/topselling_paid\",\n",
    "                    \"https://play.google.com/store/apps/collection/topgrossing\",\n",
    "                    \"https://play.google.com/store/apps/category/GAME/collection/topselling_free\",\n",
    "                    \"https://play.google.com/store/apps/category/GAME/collection/topselling_paid\",\n",
    "                    \"https://play.google.com/store/apps/category/GAME/collection/topgrossing\",\n",
    "                    \"https://play.google.com/store/apps/collection/topselling_new_free\",\n",
    "                    \"https://play.google.com/store/apps/collection/topselling_new_paid\",\n",
    "                    \"https://play.google.com/store/apps/category/GAME/collection/topselling_new_free\",\n",
    "                    \"https://play.google.com/store/apps/category/GAME/collection/topselling_new_paid\"]\n",
    "        for url in start_urls:\n",
    "            targetURL = url\n",
    "            yield  scrapy.FormRequest(               \n",
    "                     targetURL,\n",
    "                     formdata = {'start':'0',\n",
    "                                 'num':'100',\n",
    "                                 'numChildren':'0',\n",
    "                                 'cctcss':'square-cover',\n",
    "                                 'cllayout':'NORMAL',\n",
    "                                 'ipf':'1',\n",
    "                                 'xhr':'1',\n",
    "                                 'token':'zNTXc17yBEzmbkMlpt4eKj14YOo:1458833715345'},\n",
    "                    callback = self.parse_data\n",
    "                 )\n",
    "\n",
    "\n",
    "    def parse_data(self, response):  \n",
    "        table_title = response.xpath('//div[@class=\"cluster-heading\"]/h2/text()')[0].extract().strip()\n",
    "        for object_per in response.xpath('//div[@class=\"card no-rationale square-cover apps small\"]/div[@class=\"card-content id-track-click id-track-impression\"]'):\n",
    "            try:\n",
    "                title = object_per.xpath('div[@class=\"details\"]/a[@class=\"title\"]/text()')[0].extract()\n",
    "            except:\n",
    "                title = ''\n",
    "            try:\n",
    "                title_URL = 'https://play.google.com' + object_per.xpath('div[@class=\"details\"]/a/@href')[0].extract()\n",
    "            except:\n",
    "                title_URL = ''\n",
    "            try:\n",
    "                description_list = object_per.xpath('div[@class=\"details\"]/div[@class=\"description\"]/text()')[0].extract()\n",
    "                description = ''.join(description_list)\n",
    "            except:\n",
    "                description = ''\n",
    "            try:\n",
    "                autor = object_per.xpath('div[@class=\"details\"]/div[@class=\"subtitle-container\"]/a/text()')[0].extract()\n",
    "            except:\n",
    "                autor = ''\n",
    "            try:\n",
    "                star = object_per.xpath('div[@class=\"reason-set\"]/span/a/div/div/@aria-label')[0].extract()\n",
    "            except:    \n",
    "                star = 'no star_rate'\n",
    "            star_rates = star\n",
    "            \n",
    "            playitem = {}\n",
    "            playitem['title'] = title.strip()\n",
    "            playitem['title_URL'] = title_URL.strip()\n",
    "            playitem['description'] = description.strip()\n",
    "            playitem['star_rates'] = star_rates.strip()\n",
    "            playitem['table_title']= table_title.strip()\n",
    "            google_dataset.append(playitem)\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "})  \n",
    "\n",
    "process.crawl(Spider)\n",
    "process.start() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid being blocked, we change the 'User-Agent' every time we send a request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = { \"Accept\":\"text/html,application/xhtml+xml,application/xml;\",\n",
    "            \"Accept-Encoding\":\"gzip\",\n",
    "            \"Referer\":\"http://www.example.com/\" }\n",
    "\n",
    "user_agent_list = [\n",
    " 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "  'Chrome/45.0.2454.85 Safari/537.36 115Browser/6.0.3',\n",
    " 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',\n",
    " 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',\n",
    " 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)',\n",
    " 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)',\n",
    " 'Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1',\n",
    " 'Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11',\n",
    " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11',\n",
    " 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)',\n",
    " 'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0',\n",
    " 'Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1',\n",
    "]\n",
    "\n",
    "def get_soup(search_url):\n",
    "    user_agent = random.choice(user_agent_list)\n",
    "    headers['User-Agent'] = user_agent\n",
    "    \n",
    "    wait_sec = random.random()*2\n",
    "    time.sleep(wait_sec)\n",
    "    searchHtml = requests.get(search_url, headers = headers)\n",
    "    soup = BeautifulSoup(searchHtml.text, features='html5lib')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spider class. Url and title of a chart should be the input in initialization. The output of \"work\" should be a list containing items with title, description, rating, genre and the title of chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spider(object):\n",
    "    def __init__(self, dataset_json):\n",
    "        self.google_dataset = dataset_json\n",
    "            \n",
    "    def work(self):\n",
    "        result = []\n",
    "        for x in self.google_dataset:\n",
    "            url = x['title_URL']\n",
    "            soup = get_soup(url)\n",
    "            try:\n",
    "                rating = soup.find('div', {'class': 'BHMmbe'}).string.strip()\n",
    "            except:\n",
    "                rating = None\n",
    "                \n",
    "            genres = []\n",
    "            labels = soup.find_all('span', {'class': 'T32cc UAO9ie'})\n",
    "            for label in labels:\n",
    "                label_name = label.find('a').string.strip()\n",
    "                genres.append(label_name)\n",
    "            genre = ','.join(genres)\n",
    "            \n",
    "            item = x\n",
    "            item['genre'] = genre\n",
    "            item['rating'] = rating\n",
    "            \n",
    "            result.append(item)\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Final Dataset\n",
    "\n",
    "In any time failure of connection can happen. Thus we save the dataset in the format of json when we finish a chart, so that we can continue to work on the next chart after a failure of connection happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Save Final Dataset')\n",
    "\n",
    "spider = Spider(google_dataset)\n",
    "result = spider.work()\n",
    "\n",
    "with open(google_save_path, 'w') as file:\n",
    "    file.write(json.dumps(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
